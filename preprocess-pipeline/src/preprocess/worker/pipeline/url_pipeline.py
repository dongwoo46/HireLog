import logging
import json
from inputs.jd_preprocess_input import JdPreprocessInput
from url.fetcher import UrlFetcher
from url.playwright_fetcher import PlaywrightFetcher
from url.parser import UrlParser
from url.preprocessor import preprocess_url_text
from url.section_extractor import extract_url_sections
from preprocess.adapter.url_section_adapter import adapt_url_sections_to_sections
from preprocess.metadata_preprocess.metadata_preprocessor import MetadataPreprocessor
from preprocess.worker.pipeline.canonical_section_pipeline import CanonicalSectionPipeline

logger = logging.getLogger(__name__)


class UrlPipeline:
    """
    URL ê¸°ë°˜ JD ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

    í•µì‹¬ ì „ëµ (OCRê³¼ ë™ì¼):
    - URL ì „ìš© ì „ì²˜ë¦¬ë¡œ ë…¸ì´ì¦ˆ ì œê±°
    - Header ê¸°ë°˜ìœ¼ë¡œ ì„¹ì…˜ ë¶„ë¦¬ (êµ¬ì¡° í™•ì •)
    - Core/Structural ë‹¨ê³„ ì—†ì´ ë°”ë¡œ Canonicalë¡œ ì—°ê²°

    ì±…ì„:
    - URL Fetching & Parsing
    - URL ì „ìš© ì „ì²˜ë¦¬ (UI ë…¸ì´ì¦ˆ ì œê±°)
    - Header ê¸°ë°˜ ì„¹ì…˜ ë¶„ë¦¬
    - CanonicalSectionPipeline ì—°ê²°
    """

    def __init__(self):
        self.fetcher = UrlFetcher()
        self.dynamic_fetcher = PlaywrightFetcher()
        self.parser = UrlParser()
        self.metadata = MetadataPreprocessor()
        self.canonical = CanonicalSectionPipeline()

    def process(self, input: JdPreprocessInput) -> dict:
        """
        URL â†’ Fetch â†’ Parse â†’ ì „ì²˜ë¦¬ â†’ Header ë¶„ë¦¬ â†’ Canonical íë¦„ ì‹¤í–‰
        """
        if not input.url:
            raise ValueError("url is required for UrlPipeline")

        # 1ï¸âƒ£ Fetch (Hybrid Strategy)
        try:
            html_content = self.fetcher.fetch(input.url)

            # Check for JS Rendering Requirement
            if self.fetcher._needs_js_rendering(html_content):
                logger.info(f"JS Rendering required for {input.url}. Switching to Playwright.")
                html_content = self.dynamic_fetcher.fetch(input.url)

        except Exception as e:
            logger.warning(f"Static fetch failed for {input.url}, retrying with Playwright. Error: {e}")
            html_content = self.dynamic_fetcher.fetch(input.url)

        # 2ï¸âƒ£ Parse (HTML â†’ í…ìŠ¤íŠ¸)
        parsed_data = self.parser.parse(html_content)
        title = parsed_data.get("title", "")
        body_text = parsed_data.get("body", "")

        # ğŸ” DEBUG: URL fetch í›„ ì›ë³¸ ë°ì´í„°
        logger.debug(f"[URL_PIPELINE] 1ï¸âƒ£ URL fetch í›„ (url={input.url})")
        logger.debug(f"[URL_PIPELINE] title: {title}")
        logger.debug(f"[URL_PIPELINE] body_text (first 2000 chars):\n{body_text[:2000]}")

        if not body_text:
            logger.warning(f"No body text extracted from URL: {input.url}")
            return {
                "url_meta": {
                    "url": input.url,
                    "title": title,
                    "fetched_length": len(html_content),
                    "parsed_length": 0
                },
                "canonical_map": {},
                "document_meta": None,
            }

        # 3ï¸âƒ£ URL ì „ìš© ì „ì²˜ë¦¬ (ë…¸ì´ì¦ˆ ì œê±°)
        cleaned_lines = preprocess_url_text(body_text)

        # ğŸ” DEBUG: URL ì „ì²˜ë¦¬ í›„
        logger.debug("[URL_PIPELINE] 2ï¸âƒ£ URL ì „ì²˜ë¦¬ í›„ (cleaned_lines)")
        logger.debug(json.dumps(cleaned_lines, ensure_ascii=False, indent=2))

        if not cleaned_lines:
            logger.warning(f"No lines after URL preprocessing: {input.url}")
            return {
                "url_meta": {
                    "url": input.url,
                    "title": title,
                    "fetched_length": len(html_content),
                    "parsed_length": len(body_text)
                },
                "canonical_map": {},
                "document_meta": None,
            }

        # 4ï¸âƒ£ Header ê¸°ë°˜ ì„¹ì…˜ ë¶„ë¦¬ (OCR ë°©ì‹)
        raw_sections = extract_url_sections(cleaned_lines)

        # ğŸ” DEBUG: ì„¹ì…˜ ë¶„ë¦¬ í›„
        logger.debug("[URL_PIPELINE] 3ï¸âƒ£ ì„¹ì…˜ ë¶„ë¦¬ í›„ (raw_sections)")
        logger.debug(json.dumps(raw_sections, ensure_ascii=False, indent=2))

        if not raw_sections:
            logger.warning(f"No sections extracted from URL: {input.url}")
            return {
                "url_meta": {
                    "url": input.url,
                    "title": title,
                    "fetched_length": len(html_content),
                    "parsed_length": len(body_text)
                },
                "canonical_map": {},
                "document_meta": None,
            }

        # 5ï¸âƒ£ Section ë„ë©”ì¸ ê°ì²´ë¡œ ë³€í™˜
        sections = adapt_url_sections_to_sections(raw_sections)

        # 6ï¸âƒ£ Metadata ì¶”ì¶œ (ì „ì²˜ë¦¬ëœ ë¼ì¸ ê¸°ì¤€)
        document_meta = self.metadata.process(cleaned_lines)

        # 7ï¸âƒ£ Canonical í›„ì²˜ë¦¬ (Semantic â†’ Filter â†’ Canonical)
        canonical_map = self.canonical.process(sections)

        # ğŸ” DEBUG: ìµœì¢… canonical_map
        logger.debug("[URL_PIPELINE] 4ï¸âƒ£ ìµœì¢… canonical_map")
        logger.debug(json.dumps(canonical_map, ensure_ascii=False, indent=2))

        # 8ï¸âƒ£ ìµœì¢… ê²°ê³¼
        return {
            "url_meta": {
                "url": input.url,
                "title": title,
                "fetched_length": len(html_content),
                "parsed_length": len(body_text),
                "cleaned_lines_count": len(cleaned_lines),
                "sections_count": len(raw_sections),
            },
            "canonical_map": canonical_map,
            "document_meta": document_meta,
        }
